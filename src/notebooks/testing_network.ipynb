{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_otKXY13Z1US"
   },
   "source": [
    "# Get Started with pulling the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "xyneRi2qZ1Ud"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import urllib\n",
    "\n",
    "user = input('User name: ')\n",
    "password = getpass('Password: ')\n",
    "password = urllib.parse.quote(password) # your password is converted into url format\n",
    "repo_name = \"UNAST.git\"\n",
    "cmd_string = 'git clone https://{0}:{1}@github.com/{0}/{2}'.format(user, password, repo_name)\n",
    "\n",
    "!{cmd_string}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "m5ZvPma2auUn"
   },
   "outputs": [],
   "source": [
    "%cd UNAST\n",
    "!git checkout model-implementation/lucas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add parent directory to path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmtIHkAJZ1Ue"
   },
   "source": [
    "## Lets Make some dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "4lCLanS9bz09"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from module import RNNEncoder, RNNDecoder\n",
    "from network import Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "id": "-nJPK6R6Z1Uf"
   },
   "outputs": [],
   "source": [
    "# [batch_size x seq_len x hidden_dim] expected into the network\n",
    "hidden = 512\n",
    "latent = 64\n",
    "out = 100\n",
    "network_in_shape = (128, 40, 512)\n",
    "dummy = torch.randn(network_in_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUj5SLPKZ1Ug"
   },
   "source": [
    "## Let's Make a dummy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "KM0ebqQgZ1Ug"
   },
   "outputs": [],
   "source": [
    "encoder = RNNEncoder(hidden, hidden, latent, num_layers=5, bidirectional=False)\n",
    "decoder = RNNDecoder(latent, hidden, hidden, out, num_layers=5, attention=True)\n",
    "discriminator = Discriminator(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrK13mLbZ1Ug"
   },
   "source": [
    "## Now, run the network and lets see how we do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "itYpyerZcxYr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 40, 64])\n",
      "torch.Size([5, 128, 512])\n",
      "torch.Size([5, 128, 512])\n",
      "torch.Size([5, 128, 512])\n",
      "\n",
      "Discriminator output shape:\n",
      "torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "output, (latent_hidden, latent_cell) = encoder(dummy)\n",
    "print(output.shape)\n",
    "print(latent_hidden.shape)\n",
    "print(latent_cell.shape)\n",
    "print(latent_hidden.shape)\n",
    "input = latent_hidden.permute(1, 0, 2)\n",
    "\n",
    "#mask = torch.zeros(dummy.shape[0:2])\n",
    "#print(\"MASK shape\", mask.shape)\n",
    "#output_probs, hidden = decoder(input[:, -1:, :], (latent_hidden, latent_cell), output, mask)\n",
    "#print(\"\\nDecoder output shapes: \")\n",
    "#print(output_probs.shape)\n",
    "\n",
    "discriminator_out = discriminator(latent_hidden[-1])\n",
    "print(\"\\nDiscriminator output shape:\")\n",
    "print(discriminator_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Smoothed CE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_output = torch.zeros_like(discriminator_out[0])\n",
    "fake_output[:,] = torch.tensor([1,])\n",
    "fake_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_target = torch.zeros_like(fake_output)\n",
    "empty_target[:,] = torch.tensor([1,0])\n",
    "empty_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(input, target, size_average=True):\n",
    "    \"\"\" Cross entropy that accepts soft targets\n",
    "    Args:\n",
    "         pred: predictions for neural network\n",
    "         targets: targets, can be soft\n",
    "         size_average: if false, sum is returned instead of mean\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        input = torch.FloatTensor([[1.1, 2.8, 1.3], [1.1, 2.1, 4.8]])\n",
    "        input = torch.autograd.Variable(out, requires_grad=True)\n",
    "\n",
    "        target = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
    "        target = torch.autograd.Variable(y1)\n",
    "        loss = cross_entropy(input, target)\n",
    "        loss.backward()\n",
    "    \"\"\"\n",
    "    logsoftmax = nn.LogSoftmax(1)\n",
    "    if size_average:\n",
    "        return torch.mean(torch.sum(-target * logsoftmax(input), dim=1))\n",
    "    else:\n",
    "        return torch.sum(torch.sum(-target * logsoftmax(input), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6946, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(discriminator_out[0], empty_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2201)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(torch.Tensor([[1,0], [2,0]]), torch.Tensor([[1,0], [1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2201)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(torch.FloatTensor([[1,0], [2,0]]), torch.LongTensor([0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0421,  0.0095]],\n",
       "\n",
       "        [[ 0.0461,  0.0099]],\n",
       "\n",
       "        [[ 0.0388,  0.0067]],\n",
       "\n",
       "        [[ 0.0380,  0.0098]],\n",
       "\n",
       "        [[ 0.0475,  0.0055]],\n",
       "\n",
       "        [[ 0.0340,  0.0163]],\n",
       "\n",
       "        [[ 0.0406,  0.0137]],\n",
       "\n",
       "        [[ 0.0398,  0.0114]],\n",
       "\n",
       "        [[ 0.0520,  0.0111]],\n",
       "\n",
       "        [[ 0.0398,  0.0088]],\n",
       "\n",
       "        [[ 0.0405,  0.0113]],\n",
       "\n",
       "        [[ 0.0471,  0.0037]],\n",
       "\n",
       "        [[ 0.0321,  0.0103]],\n",
       "\n",
       "        [[ 0.0513,  0.0171]],\n",
       "\n",
       "        [[ 0.0418,  0.0171]],\n",
       "\n",
       "        [[ 0.0349,  0.0067]],\n",
       "\n",
       "        [[ 0.0447,  0.0115]],\n",
       "\n",
       "        [[ 0.0479,  0.0149]],\n",
       "\n",
       "        [[ 0.0479,  0.0130]],\n",
       "\n",
       "        [[ 0.0459,  0.0081]],\n",
       "\n",
       "        [[ 0.0462,  0.0046]],\n",
       "\n",
       "        [[ 0.0431,  0.0085]],\n",
       "\n",
       "        [[ 0.0385,  0.0138]],\n",
       "\n",
       "        [[ 0.0431,  0.0111]],\n",
       "\n",
       "        [[ 0.0402,  0.0062]],\n",
       "\n",
       "        [[ 0.0352,  0.0131]],\n",
       "\n",
       "        [[ 0.0413,  0.0066]],\n",
       "\n",
       "        [[ 0.0410,  0.0097]],\n",
       "\n",
       "        [[ 0.0373,  0.0197]],\n",
       "\n",
       "        [[ 0.0350,  0.0091]],\n",
       "\n",
       "        [[ 0.0386,  0.0053]],\n",
       "\n",
       "        [[ 0.0417,  0.0054]],\n",
       "\n",
       "        [[ 0.0325,  0.0084]],\n",
       "\n",
       "        [[ 0.0373,  0.0100]],\n",
       "\n",
       "        [[ 0.0442,  0.0041]],\n",
       "\n",
       "        [[ 0.0449,  0.0108]],\n",
       "\n",
       "        [[ 0.0425,  0.0028]],\n",
       "\n",
       "        [[ 0.0374,  0.0024]],\n",
       "\n",
       "        [[ 0.0380,  0.0079]],\n",
       "\n",
       "        [[ 0.0433,  0.0070]],\n",
       "\n",
       "        [[ 0.0358,  0.0146]],\n",
       "\n",
       "        [[ 0.0376,  0.0189]],\n",
       "\n",
       "        [[ 0.0473,  0.0061]],\n",
       "\n",
       "        [[ 0.0429,  0.0126]],\n",
       "\n",
       "        [[ 0.0473,  0.0109]],\n",
       "\n",
       "        [[ 0.0306,  0.0114]],\n",
       "\n",
       "        [[ 0.0385,  0.0144]],\n",
       "\n",
       "        [[ 0.0430,  0.0052]],\n",
       "\n",
       "        [[ 0.0454,  0.0124]],\n",
       "\n",
       "        [[ 0.0383,  0.0133]],\n",
       "\n",
       "        [[ 0.0409,  0.0054]],\n",
       "\n",
       "        [[ 0.0453,  0.0159]],\n",
       "\n",
       "        [[ 0.0396, -0.0034]],\n",
       "\n",
       "        [[ 0.0413,  0.0112]],\n",
       "\n",
       "        [[ 0.0383, -0.0027]],\n",
       "\n",
       "        [[ 0.0540,  0.0122]],\n",
       "\n",
       "        [[ 0.0501,  0.0121]],\n",
       "\n",
       "        [[ 0.0469, -0.0048]],\n",
       "\n",
       "        [[ 0.0421,  0.0043]],\n",
       "\n",
       "        [[ 0.0397,  0.0059]],\n",
       "\n",
       "        [[ 0.0377, -0.0002]],\n",
       "\n",
       "        [[ 0.0353,  0.0131]],\n",
       "\n",
       "        [[ 0.0437,  0.0040]],\n",
       "\n",
       "        [[ 0.0426,  0.0110]],\n",
       "\n",
       "        [[ 0.0466,  0.0078]],\n",
       "\n",
       "        [[ 0.0420,  0.0061]],\n",
       "\n",
       "        [[ 0.0440,  0.0104]],\n",
       "\n",
       "        [[ 0.0596,  0.0073]],\n",
       "\n",
       "        [[ 0.0328,  0.0031]],\n",
       "\n",
       "        [[ 0.0351,  0.0031]],\n",
       "\n",
       "        [[ 0.0384,  0.0066]],\n",
       "\n",
       "        [[ 0.0380,  0.0018]],\n",
       "\n",
       "        [[ 0.0412,  0.0133]],\n",
       "\n",
       "        [[ 0.0505,  0.0096]],\n",
       "\n",
       "        [[ 0.0337,  0.0112]],\n",
       "\n",
       "        [[ 0.0387,  0.0039]],\n",
       "\n",
       "        [[ 0.0392,  0.0117]],\n",
       "\n",
       "        [[ 0.0486,  0.0069]],\n",
       "\n",
       "        [[ 0.0491,  0.0144]],\n",
       "\n",
       "        [[ 0.0391,  0.0052]],\n",
       "\n",
       "        [[ 0.0315,  0.0119]],\n",
       "\n",
       "        [[ 0.0400,  0.0115]],\n",
       "\n",
       "        [[ 0.0326,  0.0100]],\n",
       "\n",
       "        [[ 0.0433,  0.0058]],\n",
       "\n",
       "        [[ 0.0349,  0.0119]],\n",
       "\n",
       "        [[ 0.0405,  0.0192]],\n",
       "\n",
       "        [[ 0.0426,  0.0052]],\n",
       "\n",
       "        [[ 0.0422,  0.0033]],\n",
       "\n",
       "        [[ 0.0445,  0.0047]],\n",
       "\n",
       "        [[ 0.0452,  0.0147]],\n",
       "\n",
       "        [[ 0.0448,  0.0046]],\n",
       "\n",
       "        [[ 0.0300,  0.0119]],\n",
       "\n",
       "        [[ 0.0388,  0.0056]],\n",
       "\n",
       "        [[ 0.0492,  0.0060]],\n",
       "\n",
       "        [[ 0.0450,  0.0095]],\n",
       "\n",
       "        [[ 0.0331,  0.0088]],\n",
       "\n",
       "        [[ 0.0326,  0.0023]],\n",
       "\n",
       "        [[ 0.0375,  0.0149]],\n",
       "\n",
       "        [[ 0.0435,  0.0055]],\n",
       "\n",
       "        [[ 0.0405,  0.0149]],\n",
       "\n",
       "        [[ 0.0359,  0.0116]],\n",
       "\n",
       "        [[ 0.0351,  0.0107]],\n",
       "\n",
       "        [[ 0.0392,  0.0113]],\n",
       "\n",
       "        [[ 0.0361,  0.0145]],\n",
       "\n",
       "        [[ 0.0440,  0.0054]],\n",
       "\n",
       "        [[ 0.0414,  0.0124]],\n",
       "\n",
       "        [[ 0.0481,  0.0143]],\n",
       "\n",
       "        [[ 0.0483,  0.0147]],\n",
       "\n",
       "        [[ 0.0366,  0.0188]],\n",
       "\n",
       "        [[ 0.0426,  0.0126]],\n",
       "\n",
       "        [[ 0.0401,  0.0107]],\n",
       "\n",
       "        [[ 0.0385,  0.0128]],\n",
       "\n",
       "        [[ 0.0427,  0.0125]],\n",
       "\n",
       "        [[ 0.0395,  0.0063]],\n",
       "\n",
       "        [[ 0.0386,  0.0050]],\n",
       "\n",
       "        [[ 0.0381,  0.0143]],\n",
       "\n",
       "        [[ 0.0418,  0.0110]],\n",
       "\n",
       "        [[ 0.0353,  0.0125]],\n",
       "\n",
       "        [[ 0.0529,  0.0075]],\n",
       "\n",
       "        [[ 0.0458,  0.0088]],\n",
       "\n",
       "        [[ 0.0444,  0.0073]],\n",
       "\n",
       "        [[ 0.0466,  0.0063]],\n",
       "\n",
       "        [[ 0.0417,  0.0030]],\n",
       "\n",
       "        [[ 0.0374,  0.0157]],\n",
       "\n",
       "        [[ 0.0448,  0.0076]],\n",
       "\n",
       "        [[ 0.0399,  0.0134]],\n",
       "\n",
       "        [[ 0.0421,  0.0011]],\n",
       "\n",
       "        [[ 0.0374,  0.0097]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "testing_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "speech"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
